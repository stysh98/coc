{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dG063WMBO1ac",
    "outputId": "c5c758a1-7e8c-46fc-d060-64e257000260"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m36.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m51.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m82.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "pip install -q transformers accelerate peft bitsandbytes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 508,
     "referenced_widgets": [
      "8f4d228e06c940af94fbdda6af087bda",
      "3c24f5d168a54551a7fc3c98f13e36b3",
      "516d623407124b5ba3cda380864a5439",
      "63eb5862e1784e61b929337d480def71",
      "d8c183fadc5c4b02bd9e29a29e47804e",
      "d92f12661294488e80082acd90b98bd5",
      "f9794f79bae64e309653c9aabeda1e1c",
      "60757cb347904e52bb91bdbb4c90db33",
      "1cca82e69d3445ac80ef683423753a7a",
      "94c7257b02014e3c84de979b035c78f8",
      "2a6fd981b64e48ac9c57b7dd7d602a87",
      "e538754b70a542a6b6812e4bc8cd35ea",
      "99207a60d065473488b0521b87c22c81",
      "0b59771514bf4228becb07022f32fe94",
      "2f1469bc50304669a045d0b015aa3e19",
      "7d1b4b6b732a40fe8bb740189ad18653",
      "937d5cd807a147e7b86a923e2f159e9e",
      "cec87e0eec8342ed869cf7e200acd07f",
      "afd45a6b49eb4cd4ae68b00215d5f50e",
      "6006f58a94c94b42a7f759d3d24a99f2",
      "a92f31e9467f4cfe8cc4a499df67e928",
      "a1fb41252dc7443881687b772fcf0027",
      "16a5499fbad7451e9a730843603c9650",
      "3117ebac64fa408d8f39ca1ef5f8919e",
      "98e22f2dac6b4d07a86c5ce61be8ce27",
      "dcae2310b6f143c18525af0f086a4cd6",
      "7f5e82fb5b264ec18cab7f65451cfef2",
      "f0b598dd587c4fd2bba25f81b2acfbd7",
      "bace3954d3b147b8a88a0e2dff68205a",
      "6accfff800014404a316f5a7aa2fbb8e",
      "acff8997ea3949a7b28f5c8acbad3c62",
      "ca0b107e6d884cf9bed35894bbb8db1b",
      "292fe67853af4aa5a02d732a324b32fe",
      "ae7cd838d7334ad88584f60c4296af0f",
      "ac4659cddea444928892e32f49692e37",
      "9dd1761cb0ef484c891f9a809e795a2e",
      "ea5140e4a29f48f48c557b3906ed1cc3",
      "61d7eb3f41d74eaea14bb8ee38117528",
      "418876c6187a4e2a87a64907bee387d8",
      "966614ef414f4c5aa556dab3e6a0f857",
      "d347a9a7bf704dab879aec02484befa5",
      "e23a4f101e5b4f019df19380df67afed",
      "0c57110f565843d0bf0309a6271db4d6",
      "6ab83f770dbc4195bd673793ae9ad7b7",
      "2edac61ce07545d88251e606861d8ad9",
      "7cf9487da063485ab0d95a9f6338ab86",
      "0115c3f5f5c0407f9e10307f9e150fee",
      "ae60776b03494eac920c33453c0de8b1",
      "a1e37e054d534da39a4804632af49b58",
      "61b45ae1ee094a208f8ecb305cf7dc2d",
      "31224e31a4bd469891a68cc303825973",
      "eb85954953834e56a33ef5e193c0ad6b",
      "c908c6df98794f59819e1b7681a445f2",
      "6d866146e52a43ebb0b6b2614ec6150c",
      "c031e694a89f4b399cdbcbd1a6faa06b",
      "f8b7ce53000a45269badf24fd9728df0",
      "0fb0af71ff274620907dc1cd850007c2",
      "afd16e2c0d8a4113aa346f6a79146963",
      "d8bec2f4333d4e47b1f19f135c6e2c73",
      "3f42f5ac48f744a1a31a2a7f9808f3c3",
      "68af9273fcec4034a033fbd0fe9e4014",
      "c7ec3565855a41cdaff24e4d6f7f5dff",
      "d20cfe19f2ed4326b43e6a4cfa945b74",
      "7972dff660284f35bd96fb97bb83037a",
      "b6ad4ceebe614f33a68b25bc64ce5b69",
      "73a806226246481588f210d02f5acd55",
      "a28f5e45a91945e2a50c685a0cde8c96",
      "a586c7e8f2764f85b5c8f1334b54215f",
      "db53d2aa4cec43cd829745915ff7a237",
      "98e556543abd47b89cc17914d35afbbc",
      "c6ba9227705e46a68042afea044151d4",
      "ff6c97f652f64ca9b53edaf33eae0e67",
      "c4f27a7f8e8f4970a67031c6feb2b94a",
      "043d6b6212ab487cb14eaa03d2496538",
      "4fa72f38e557405b8d8229ea8e3e445b",
      "26d5cda558ee47488759a7562b1e8333",
      "6d4659d3bad84902a714ee28addf9d04",
      "def615d578594ababb92bc242443dcd4",
      "db0eb20b45684d1698da1b92a854ece8",
      "81d71ed3a41d48d1a3fea7b2e1dc93cf",
      "002235b35a2b4795842d730474563d71",
      "421a04aa1401466ab5ddf7953a48e082",
      "a19fcbc303374afea4c04cc0f6717882",
      "5f55fd2b9fd74be9803bddea734e03a0",
      "93562d028ce941dfbc944892bbd027d6",
      "841d90628576437f830aaf27d0b7d721",
      "c904e420d4d44d78ab2614f300bcf219",
      "b2fde6da3cc047b2837239f500a41376",
      "5f979b4e4cc7441bb19b9f546300c8a7",
      "8b7e28ccad6843c5a63a6c6e2bb979b0",
      "327ad85e96824109b73354e01b017f84",
      "87c77891e08b4810b16715ab7114ba14",
      "e6df6c37add94417b72ba4d1b249d261",
      "328b570eda994e42b2a23b7a60647ee0",
      "fb633f69ecbb460a9f50c9eb038ac234",
      "de99e73311f94066adbccc99e422e385",
      "207bf3351e484be69a74866c189a9bc5",
      "f7ac2d1820de4c73aa64cc433ec7ef7d",
      "1bd9760150954ee9b4177d5eff968213",
      "ffeadfca84554b06883c1e87cda02f1b",
      "f0f0ea9921ee4044a4e258fc5606c046",
      "a9860b691bdf42c5affac02e6b01144b",
      "fb993dcae11245c187a031609167868a",
      "5335252136f542f69a42535e53987a54",
      "d4dd266d19744c3ca5684b367b7e2074",
      "f874e5b840e14fa081286922891f49e6",
      "b3cad74aa0f44f7cb5bc44527d91fffa",
      "574e852bda1a4485bfa59ca1cd014742",
      "d5cb047527ab47dbbf0c5e6fa3f758ee",
      "085f2ef1601d407e941f0d7b3fc752d8",
      "7047f27b19d5485b8327a34b1a69feef",
      "7ab796b47b7b4760933fec4b85d90d75",
      "09eab21c0ce04ce7add5f29f40452cb2",
      "aa7b4e9ffbb64016977e6efc183aba02",
      "48d1cb258fd74696889909b1abea353a",
      "7e3d2e623fc2413a9f3172cefb4970c3",
      "3d7381d65d304b04866613c4f6689e88",
      "fe922b2829fa49a5b71adda28921a9d9",
      "e02cb2b1f66d4f7a863b896659793278",
      "f13503f940ca4bf382d101f102bcd71d",
      "c1dbf41750ed49918f8fcd728e1cda8a"
     ]
    },
    "id": "oHbiGKXYPpwN",
    "outputId": "2cc9fcf2-b3f0-4077-f61a-f6f9011c2b09"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f4d228e06c940af94fbdda6af087bda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/2.10k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e538754b70a542a6b6812e4bc8cd35ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16a5499fbad7451e9a730843603c9650",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.80M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae7cd838d7334ad88584f60c4296af0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2edac61ce07545d88251e606861d8ad9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8b7ce53000a45269badf24fd9728df0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/25.1k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a28f5e45a91945e2a50c685a0cde8c96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "def615d578594ababb92bc242443dcd4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/9.94G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f979b4e4cc7441bb19b9f546300c8a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/4.54G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffeadfca84554b06883c1e87cda02f1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7047f27b19d5485b8327a34b1a69feef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:accelerate.big_modeling:Some parameters are on the meta device because they were offloaded to the cpu.\n",
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "import torch\n",
    "\n",
    "# Load model and tokenizer\n",
    "model_id = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# Create text generation pipeline\n",
    "generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "my8tAkjmR1rZ",
    "outputId": "052d609e-851d-494f-dac8-860d9e5cccee"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: Define a Python dictionary named 'objects' that contains:\n",
      "- \"orange\": 1\n",
      "- \"violin\": 1\n",
      "- \"apple\": 1\n",
      "Just write the code.\n",
      "A: objects = {\"orange\": 1, \"violin\": 1, \"apple\": 1}\n"
     ]
    }
   ],
   "source": [
    "prompt_1 = \"\"\"Q: Define a Python dictionary named 'objects' that contains:\n",
    "- \"orange\": 1\n",
    "- \"violin\": 1\n",
    "- \"apple\": 1\n",
    "Just write the code.\"\"\"\n",
    "\n",
    "output_1 = generator(prompt_1, max_new_tokens=50, do_sample=False)[0][\"generated_text\"]\n",
    "print(output_1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6pp2jrLPR6Au",
    "outputId": "d2037a46-1812-4684-d4b8-1bc53612aaae"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: Write a Python function named 'is_fruit' that takes an object name and returns True if it is a fruit.\n",
      "The valid fruits are: orange, apple, banana, peach, plum, grape, pear, cherry.\n",
      "Just write the function.\n",
      "\n",
      "A: def is_fruit(name):\n",
      "    fruits = ['orange', 'apple', 'banana', 'peach', 'plum', 'grape', 'pear', 'cherry']\n",
      "    if name.lower() in fruits:\n",
      "        return True\n",
      "    else:\n",
      "        return False\n"
     ]
    }
   ],
   "source": [
    "prompt_2 = \"\"\"Q: Write a Python function named 'is_fruit' that takes an object name and returns True if it is a fruit.\n",
    "The valid fruits are: orange, apple, banana, peach, plum, grape, pear, cherry.\n",
    "Just write the function.\"\"\"\n",
    "\n",
    "output_2 = generator(prompt_2, max_new_tokens=80, do_sample=False)[0][\"generated_text\"]\n",
    "print(output_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RIlLyswBZLvD",
    "outputId": "a72adacd-bb39-4f99-f3f5-8854ea20c26e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: Suppose you have a dictionary named 'objects' and a function 'is_fruit'.\n",
      "Write a Python for-loop to count how many of the items in the dictionary are fruits. Store the result in 'num_fruits'.\n",
      "Just write the loop code.\n",
      "A:\n",
      "```\n",
      "num_fruits = 0\n",
      "for item in objects:\n",
      "    if is_fruit(item):\n",
      "        num_fruits += 1\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "prompt_3 = \"\"\"Q: Suppose you have a dictionary named 'objects' and a function 'is_fruit'.\n",
    "Write a Python for-loop to count how many of the items in the dictionary are fruits. Store the result in 'num_fruits'.\n",
    "Just write the loop code.\"\"\"\n",
    "\n",
    "output_3 = generator(prompt_3, max_new_tokens=80, do_sample=False)[0][\"generated_text\"]\n",
    "print(output_3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Uc24z4CrZVgv",
    "outputId": "99b3d26b-5db0-43b2-a516-1424b7a8da5f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: Given that you have calculated the number of fruits and stored it in 'num_fruits', print the result in a sentence.\n",
      "Just write the print line.\n",
      "A: \"There are {} fruits in the basket.\".format(num_fruits)\n"
     ]
    }
   ],
   "source": [
    "prompt_4 = \"\"\"Q: Given that you have calculated the number of fruits and stored it in 'num_fruits', print the result in a sentence.\n",
    "Just write the print line.\"\"\"\n",
    "\n",
    "output_4 = generator(prompt_4, max_new_tokens=30, do_sample=False)[0][\"generated_text\"]\n",
    "print(output_4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WPATEqyjaeoe",
    "outputId": "fcf85a18-af92-48ab-f2c4-ac38087653ec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 2 fruits in the basket.\n"
     ]
    }
   ],
   "source": [
    "# Step 1\n",
    "objects = {\"orange\": 1, \"violin\": 1, \"apple\": 1}\n",
    "\n",
    "# Step 2\n",
    "def is_fruit(name):\n",
    "    fruits = ['orange', 'apple', 'banana', 'peach', 'plum', 'grape', 'pear', 'cherry']\n",
    "    if name.lower() in fruits:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "# Step\n",
    "num_fruits = 0\n",
    "for item in objects:\n",
    "    if is_fruit(item):\n",
    "        num_fruits += 1\n",
    "\n",
    "# Step\n",
    "print(\"There are {} fruits in the basket.\".format(num_fruits))\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
